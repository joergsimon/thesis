{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of values for channels and sensors\n",
    "\n",
    "An interesting observation is how the values are distributed for the different channels and sensor if we ignore the temporal relationship. If these distributions are very different this is a hind that there is already some general structural difference which can likely be used for a classifyer. The reverse is not nessecarely true. Two very similar distributions ignoring the temporal relationship can still be very different when we put the timing back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import gestureanalysis.specific_utils as sutils\n",
    "from gestureanalysis.constants import Constants\n",
    "from typing import List, Callable\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/jsimon/Documents/thesis/gesture-analysis/data/\"\n",
    "time_groups_path_corrected_pickl = base_path+\"transformed/time_added/all/time-and-groups-corrected-all.pkl\"\n",
    "stats_added_path_pickl = base_path+\"transformed/stats_added/all/raw_stats-added-all.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jsimon/Documents/thesis/gesture-analysis/scripts'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check working directory and adopt if needed\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you need to reload, and know it exists:\n",
    "with open( time_groups_path_corrected_pickl, \"rb\" ) as users_pickle_file:\n",
    "    users = pickle.load(users_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = users.keys()\n",
    "gestures = users['AB73']['label'][0]['data']['gesture'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumerate all the channels\n",
    "\n",
    "Let's see what channels we have. We have 63 channels with sensor data. The last columns from the dataset are a erroneous magnetometer in x,y,z and the labels. The other channels are somewhat mixed up. Generally the first channels are flex sensors with the exception of 1_Thumb_pressure which is a pressure sensor. The other pressure sensors are from 12_Finger_1_pressure to 15_Finger_4_pressure followed by two more flex sensors on the wrist. From that we always have triplets of an accelerometer x,y,z followed by a gyroscope x,y,z triplet for each of the IMUs on the glove. We finish with the magnetometer x,y,z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0_Thumb_base', '1_Thumb_pressure', '2_Angle_between_thumb_and_hand',\n",
       "       '3_Finger_1_base', '4_Finger_1_tip', '5_Finger_2_base',\n",
       "       '6_Finger_2_tip', '7_Finger_3_base', '8_Finger_3_tip',\n",
       "       '9_Finger_4_base', '10_Finger_4_tip', '11_Thumb_tip',\n",
       "       '12_Finger_1_pressure', '13_Finger_2_pressure', '14_Finger_3_pressure',\n",
       "       '15_Finger_4_pressure', '16_Wrist_extension', '17_Wrist_flexion',\n",
       "       '18_Finger_1_Accel_X', '19_Finger_1_Accel_Y', '20_Finger_1_Accel_Z',\n",
       "       '21_Finger_1_Gyro_X', '22_Finger_1_Gyro_Y', '23_Finger_1_Gyro_Z',\n",
       "       '24_Finger_2_Accel_X', '25_Finger_2_Accel_Y', '26_Finger_2_Accel_Z',\n",
       "       '27_Finger_2_Gyro_X', '28_Finger_2_Gyro_Y', '29_Finger_2_Gyro_Z',\n",
       "       '30_Finger_3_Accel_X', '31_Finger_3_Accel_Y', '32_Finger_3_Accel_Z',\n",
       "       '33_Finger_3_Gyro_X', '34_Finger_3_Gyro_Y', '35_Finger_3_Gyro_Z',\n",
       "       '36_Finger_4_Accel_X', '37_Finger_4_Accel_Y', '38_Finger_4_Accel_Z',\n",
       "       '39_Finger_4_Gyro_X', '40_Finger_4_Gyro_Y', '41_Finger_4_Gyro_Z',\n",
       "       '42_Thumb_Accel_X', '43_Thumb_Accel_Y', '44_Thumb_Accel_Z',\n",
       "       '45_Thumb_Gyro_X', '46_Thumb_Gyro_Y', '47_Thumb_Gyro_Z',\n",
       "       '48_Palm_Accel_X', '49_Palm_Accel_Y', '50_Palm_Accel_Z',\n",
       "       '51_Palm_Gyro_X', '52_Palm_Gyro_Y', '53_Palm_Gyro_Z',\n",
       "       '54_Wrist_Accel_X', '55_Wrist_Accel_Y', '56_Wrist_Accel_Z',\n",
       "       '57_Wrist_Gyro_X', '58_Wrist_Gyro_Y', '59_Wrist_Gyro_Z',\n",
       "       '60_Magnetometer_X', '61_Magnetometer_Y', '62_Magnetometer_Z',\n",
       "       '63_Magnetometer_X_ignore_double', '64_Magnetometer_Y_ignore_double',\n",
       "       '65_Magnetometer_Z_ignore_double', 'label_automatic', 'label_manual',\n",
       "       'label_dynamic', 'label_static'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = users['AB73']['glove_merged'].columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the channels are not too well structured I created a lookup dictionary with the indexes of the channels groups around some concepts. You find all the indices of the individual sensors in it, as you find anatomic concept like all the data of finger 1 or the thumb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['flex', 'pressure', 'accel', 'gyro', 'magnetometer', 'lin_accel', 'thumb', 'finger_1', 'finger_2', 'finger_3', 'finger_4', 'wrist', 'palm'])\n"
     ]
    }
   ],
   "source": [
    "idx_keys = Constants().raw_indices.keys()\n",
    "print(idx_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the distribution for outliers\n",
    "\n",
    "A classical thing is to explore the disbribution for outliers. In our case we assume outliers come from bad sensors. If that is true, tue to the distinctive nature of ouliers being seldom and good to detect, outliers bear the danger to be picked up as features for detecting certain gestures who just by chance happen to have outliers in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_percentile = 98.5\n",
    "lower_percentile = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_value_range(columes, remove_outliers, show_overal):\n",
    "    def describe_values(line, username):\n",
    "        print('user: ', username)\n",
    "        print(pd.DataFrame(data=line).describe())\n",
    "        print(\"\")\n",
    "    all_vals = sutils.collect_values(usernames, users, columes, remove_outliers, \n",
    "                                     higher_percentile, lower_percentile, \n",
    "                                     True, describe_values, use_tqtm=True)\n",
    "    if show_overal:\n",
    "        print(pd.DataFrame(data=all_vals).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_values(columes):\n",
    "    all_vals = sutils.collect_values(usernames, users, columes, False, \n",
    "                                     None, None, False, None, use_tqtm=True)\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_(X):\n",
    "    print('.... start finding outliers ....')\n",
    "    #clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1, n_jobs=3)\n",
    "    clf = IsolationForest(max_samples=100, n_jobs=3)\n",
    "    print('.... fit ....')\n",
    "    clf.fit(X)\n",
    "    print('.... predict ....')\n",
    "    y_pred = clf.predict(X)\n",
    "    print('.... done ....')\n",
    "    #X_scores = clf.negative_outlier_factor_\n",
    "    return y_pred #, X_scores\n",
    "\n",
    "def visualise_outliers(X, y_pred, X_scores):\n",
    "    plt.title(\"Local Outlier Factor (LOF)\")\n",
    "    plt.scatter(X, color='k', s=3., label='Data points')\n",
    "    # plot circles with radius proportional to the outlier scores\n",
    "    radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\n",
    "    plt.scatter(X, s=1000 * radius, edgecolors='r',\n",
    "            facecolors='none', label='Outlier scores')\n",
    "    plt.axis('tight')\n",
    "    plt.xlim((-5, 5))\n",
    "    plt.ylim((-5, 5))\n",
    "    plt.xlabel(\"prediction errors: %d\" % (n_errors))\n",
    "    legend = plt.legend(loc='upper left')\n",
    "    legend.legendHandles[0]._sizes = [10]\n",
    "    legend.legendHandles[1]._sizes = [20]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['0_Thumb_base', '2_Angle_between_thumb_and_hand', '3_Finger_1_base',\n",
      "       '4_Finger_1_tip', '5_Finger_2_base', '6_Finger_2_tip',\n",
      "       '7_Finger_3_base', '8_Finger_3_tip', '9_Finger_4_base',\n",
      "       '10_Finger_4_tip', '11_Thumb_tip', '16_Wrist_extension',\n",
      "       '17_Wrist_flexion'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "all_flex = Constants().raw_indices['flex']['all']\n",
    "print(cols[all_flex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90180c53db1413b92841abc97464a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping userAE30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = get_all_values(cols[all_flex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... start finding outliers ....\n",
      ".... fit ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsimon/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/ensemble/iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "/home/jsimon/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/ensemble/iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6dc3a440bbf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_outliers_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-029656a044d8>\u001b[0m in \u001b[0;36mfind_outliers_\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIsolationForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.... fit ....'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.... predict ....'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/ensemble/iforest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             self._threshold_ = np.percentile(self.decision_function(X),\n\u001b[0m\u001b[1;32m    274\u001b[0m                                              100. * self._contamination)\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/ensemble/iforest.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;31m# an outlier:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/ensemble/iforest.py\u001b[0m in \u001b[0;36mscore_samples\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mn_samples_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mdepths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_pred = find_outliers_(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_outliers(X, y_pred, X_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
